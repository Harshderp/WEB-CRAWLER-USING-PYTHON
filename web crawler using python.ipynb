{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB CRAWLER USING PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in this notebook i will describe 'How to make a web crawler using python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re    \n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to build a web crawler i am using three in-built libraries of python.\n",
    "### 1) requests\n",
    "### 2) re\n",
    "### 3) urlparse\n",
    "### And the first step is to import all these libraries(code written above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visited = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am using a set named visited and i will discuss the use of this set later in this notebook(code written above) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    try:\n",
    "        html = requests.get(url, timeout = 5)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return ''\n",
    "    return html.content.decode('latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For finding all the links in a webpage it's important to get the source code of that page. So, we can find all the links present in that page and for do the same i had defined a function i.e get_page.\n",
    "### The main functionality of this page is to return the source code of that page(code written above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links(url): \n",
    "    html = get_page(url)    \n",
    "    parsed = urlparse(url)    \n",
    "    base = f\"{parsed.scheme}://{parsed.netloc}\"    \n",
    "    links = re.findall('''<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"''', html)    \n",
    "    for i, link in enumerate(links):    \n",
    "        if not urlparse(link).netloc:    \n",
    "            link_with_base = base + link    \n",
    "            links[i] = link_with_base    \n",
    "\n",
    "    return set(filter(lambda x: 'mailto' not in x, links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now as we have the source code of the webpage it's time to get all the links from that source code.In order to extract all the links i had defined a function get_links(code written above)\n",
    "### The main functionality of this function is to return a set including all the links available in that source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_info(url):    \n",
    "    html = get_page(url)    \n",
    "    meta = re.findall(\"<meta .*?name=[\\\"'](.*?)['\\\"].*?content=[\\\"'](.*?)['\\\"].*?>\", html)    \n",
    "    return dict(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when you search anything on google you might had seen that google provides the webpage links with their description(content in that page). To do the same i had defined a function extract_info(code written above).\n",
    "### The main functionality of this function is to return a dictionary containing description, keywords in a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl(url):    \n",
    "    for link in get_links(url):    \n",
    "        if link in visited:    \n",
    "            continue    \n",
    "        visited.add(link)    \n",
    "        info = extract_info(link)    \n",
    "\n",
    "        print(f\"\"\"Link: {link}    \n",
    "Description: {info.get('description')}    \n",
    "Keywords: {info.get('keywords')}    \n",
    "            \"\"\")    \n",
    "        crawl(link) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we have extracted all the links and we have their description also it's time to present(display) them in a good way. to do the same i had define a function crawl(code written above).\n",
    "### It uses the visited (set defined at the start of this notebook) and store all the links it visit along with their info and one another functionality of this function is that it will never return the same link again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FULL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re    \n",
    "from urllib.parse import urlparse\n",
    "\n",
    "visited = set()\n",
    "\n",
    "def get_page(url):\n",
    "    try:\n",
    "        html = requests.get(url, timeout = 5)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return ''\n",
    "    return html.content.decode('latin-1')\n",
    "\n",
    "def get_links(url): \n",
    "    html = get_page(url)    \n",
    "    parsed = urlparse(url)    \n",
    "    base = f\"{parsed.scheme}://{parsed.netloc}\"    \n",
    "    links = re.findall('''<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"''', html)    \n",
    "    for i, link in enumerate(links):    \n",
    "        if not urlparse(link).netloc:    \n",
    "            link_with_base = base + link    \n",
    "            links[i] = link_with_base    \n",
    "\n",
    "    return set(filter(lambda x: 'mailto' not in x, links))\n",
    "\n",
    "def extract_info(url):    \n",
    "    html = get_page(url)    \n",
    "    meta = re.findall(\"<meta .*?name=[\\\"'](.*?)['\\\"].*?content=[\\\"'](.*?)['\\\"].*?>\", html)    \n",
    "    return dict(meta)\n",
    "\n",
    "def crawl(url):    \n",
    "    for link in get_links(url):    \n",
    "        if link in visited:    \n",
    "            continue    \n",
    "        visited.add(link)    \n",
    "        info = extract_info(link)    \n",
    "\n",
    "        print(f\"\"\"Link: {link}    \n",
    "Description: {info.get('description')}    \n",
    "Keywords: {info.get('keywords')}    \n",
    "            \"\"\")    \n",
    "        crawl(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING THE CRAWLER(RUN BELOW CODE FOR TESTING THE WEB CRAWLER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crawl('https://www.crawler-test.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note : This  above code is for crawling a webpage using your own ip address\n",
    "# Stay Connected If you want to know 'How to make a web crawler using proxy server and python'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
